Logistic regression
Logistic regression is a supervised machine learning algorithm used for classification tasks. It models the probability of a binary outcome Logistic regression works by fitting a logistic function to the data. The logistic function is a sigmoid function that transforms the linear combination of the independent variables into a probability score.

Assumptions
The independent variables are linearly related to the logit of the dependent variable.
The independent variables are independent of each other.
The errors are independent and identically distributed

Naive Bayes classifier
A Naive Bayes classifier is a supervised machine learning algorithm that is based on Bayes' theorem Naive Bayes classifiers work by assuming that the features of the input data are conditionally independent given the class To classify a new data point, a Naive Bayes classifier first calculates the probability of each class given the data point's features. This is done using Bayes' theorem
					P(A/B) = P(B/A) P(A) / P(B)

Advantages:
Simple to understand and implement.
Can be trained quickly and efficiently.
Not sensitive to irrelevant features.
Can handle both continuous and discrete data

Disadvantages:
Make the assumption that the features of the input data are conditionally independent, which is often unrealistic.
Can be susceptible to overfitting, especially if the training data is small.

K-Nearest Neighbor (KNN)
KNN is a supervised machine learning algorithm that can be used for both classification and regression tasks. It works by finding the K most similar data points (neighbors) to a new data point and then predicting the class or value of the new data point based on the classes or values of its neighbors

The KNN algorithm works in two phases: training and prediction
In the training phase, the KNN algorithm simply stores the entire training dataset in memory.
Prediction phase
Calculates the distance between the new data point and all of the training data points.
Selects the K most similar training data points to the new data point (the K nearest neighbors).
Predicts the class or value of the new data point based on the classes or values of its K nearest neighbors

Advantages:
KNN is a simple and easy-to-understand algorithm.
KNN is a very versatile algorithm that can be used for both classification and regression tasks.
KNN is a non-parametric algorithm, which means that it does not make any assumptions about the underlying distribution of the data.
KNN is relatively robust to outliers and noise.

Disadvantages:
KNN can be computationally expensive, especially for large datasets.
KNN is sensitive to the choice of the distance metric.
KNN is prone to overfitting if the value of K is not chosen carefully.


Decision Tree
A decision tree is a tree-like structure that represents a series of decisions and their possible consequences. It is used in machine learning for classification and regression tasks. An example of a decision tree is a flowchart that helps a person decide what to wear based on the weather conditions

Types-Classification tree, Regression tree, Cost-complexity pruning tree, and Reduced Error Pruning tree.
Tree algorithm-ID3, C4.5, CART, and Random Forest
The goal of machine learning is to decrease uncertainty or disorders from the dataset and for this, we use decision trees.
Entropy-the uncertainty in our dataset or measure of disorder.choose low entropy
the higher the Entropy, the lower will be the purity and the higher will be the impurity.

Gini impurity measures the probability of misclassifying a randomly chosen element, while entropy measures the 	amount of disorder or randomness in a datase

Gini Index-works with the categorical target variable “Success” or “Failure”. It performs only Binary splits.
Lower Gini impurity indicates a better split

Information gain-which tells us how much the parent entropy has decreased after splitting it with some feature.Measures the reduction in entropy or impurity achieved by a split. A higher information gain suggests a better split.
		It is just entropy of the full dataset – entropy of the dataset given some feature.

Gain Ratio- measure that takes into account both the information gain and the number of outcomes of a feature to determine the best feature to split on     Gain Ratio=information gain / splitinfo

Pruning-is another method that can help us avoid overfitting. It helps in improving the performance of the tree by cutting the nodes or sub-nodes which are not significant. Additionally, it removes the branches which have very low importance.

Pre-pruning – we can stop growing the tree earlier, which means we can prune/remove/cut a node if it has low importance while growing the tree.
Post-pruning – once our tree is built to its depth, we can start pruning the nodes based on their significance.

Advantages:-
Easy to understand and interpret.
Can handle both numerical and categorical data.
Require minimal data preprocessing.

Disadvantages:-
Prone to overfitting.
Can be unstable with small variations in the data.
May not always find the globally optimal tree.

Random Forest Algorithm
A Random Forest is an ensemble learning method that consists of multiple Decision Trees. It differs from a single Decision Tree in that it reduces overfitting by aggregating predictions from multiple trees, which are trained on different subsets of the data with bootstrapping and feature randomization.

Bagging– It creates a different training subset from sample training data with replacement & the final output is based on majority voting. For example,  Random Forest.
Boosting– It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example,  ADA BOOST, XG BOOST
Boosting is one of the techniques that use the concept of ensemble learning. A boosting algorithm combines multiple simple models (also known as weak learners or base estimators) to generate the final output. It is done by building a model by using weak models in series.

OOB error - It is used as an estimate of the model's performance on unseen data without the need for a separate validation set.


Advantages-
Reduced overfitting.
Improved generalization.
Robust to noisy data.
Provides feature importance scores..
Disadvantages:-
Increased complexity.
Longer training time due to multiple trees.
May not perform well on very high-dimensional data.

SVM
SVM algorithm is used for both classification and regression tasks. It finds an optimal hyperplane to separate data points of different classes in a high-dimensional space.
SVM is considered one of the best algorithms because it can handle high-dimensional data, is effective in cases with limited training samples, and can handle non-linear classification using kernel functions

Support Vectors: These are the points that are closest to the hyperplane. A separating line will be defined with the help of these data points.
Margin: The margin in SVMs is the distance between the hyperplane and the closest data points from each class. 	large margin is considered a good margin
Hard margin-If our data is linearly separable, we go for a hard margin. 
Soft margin-but the margin is so small that the model becomes prone to overfitting or being too sensitive to outliers. Also, in this case, we can opt for a larger margin by using soft margin SVM in order to help the model generalize better.

Kernal:-we do is try converting this lower dimension space to a higher dimension space using some quadratic 	functions, SVM can handle non-linearly separable data using the kernel trick., Kernels (e.g., Polynomial, Radial Basis Function - RBF, Sigmoid) transform the input data into a higher-dimensional space where it might become linearly  separable., The kernel function replaces the dot product of feature vectors in the original space with a similarity measure in the higher-dimensional space.

C parameter-The C parameter in SVMs controls the trade-off between maximizing the margin and minimizing classification errors
Advantages of SVM
Effective in high-dimensional spaces.
Robust against overfitting, especially with the right choice of kernel.
Suitable for both linear and non-linear classification

Disadvantages of SVM
Can be computationally expensive for large datasets.
Choice of kernel and hyperparameter tuning can be challenging.
Not well-suited for multi-class problems with many classes.

Accuracy: correct predictions made by the model.   
	 Accuracy = (Number of correct predictions) / (Total number of predictions)
Precision: Precision is the proportion of positive predictions that are actually correct.
		Precision = (Number of true positives) / (Number of predicted positives)
Recall :is the proportion of actual positives that are correctly identified as such by the model.
			Recall = (Number of true positives) / (Number of actual positives)

F1-Score: The F1 score is the harmonic mean of precision and recall. It is a good measure of a model's overall performance when both precision and recall are important.	
			F1 score = 2 * (Precision * Recall) / (Precision + Recall)
Confusion Matrix:A confusion matrix is a table used in machine learning and statistics to assess the performance of a classification model. It summarizes the results of classification by showing the counts of true positive, true negative, false positive, and false negative predictions

AUC :is a measure of a model's overall performance on the ROC curve. A higher AUC indicates a better model.

ROC: reciver operator curve that plots the true postive rate against false postive rate

 