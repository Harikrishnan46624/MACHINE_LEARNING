Lasso Regression:
Lasso regression, also known as Least Absolute Shrinkage and Selection Operator, is a type of linear regression that uses L1 regularization to shrink the coefficients of less important features towards zero. This can help to reduce overfitting and improve the interpretability of the model

This means that it can automatically select the most important features for prediction and shrink the coefficients of the less important features towards zero. This can help to improve the accuracy and interpretability of the model.

use case - Lasso regression is particularly useful when you have a dataset with a large number of features
		L2 = RSS(w) + alpha * (sum of the absolute value of coefficient)²
Lasso Regression is useful for feature selection and dimensionality reduction. It can also be used to reduce overfitting and improve the interpretability of linear regression models.

parameter
Alpha (or Lambda)
Fit Intercept
Normalize  
max iter 
selection

Advantages:
Feature Selection
Regularization
Interpretability
Suitable for High-Dimensional Data

Disadvantages:
Loss of Information: 
Sensitive to Hyperparameter
Not Ideal for All Data
Computationally Intensive:

Polynomial Regression:
Polynomial regression is a type of machine learning algorithm used to model nonlinear relationships between independent and dependent variables. It is a generalization of linear regression, which can only model linear relationships
Polynomial regression works by fitting a polynomial function to the data. The degree of the polynomial determines the complexity of the model. A higher degree polynomial will be able to model more complex relationships, but it will also be more prone to overfitting.

Types of Polynomial Regression
Linear – if degree as 1
Quadratic – if degree as 2
Cubic – if degree as 3
 
parameters:
Degree
include bias
order
intraction only

Advantages
Polynomial is a type of curve that can accommodate a wide variety of curvatures.
Captures Nonlinear Relationships
Flexible Modeling
Simple to Implement
Interpretability

Disadvantages
One or two outliers in the data might have a significant impact on the nonlinear analysis’ outcomes
Overfitting
Complexity
Sensitive to Outliers
Increased Variance

Support vector regression (to handle non-linear relationships between input features and target values)
Support vector regression (SVR) is a supervised machine learning algorithm used for regression tasks. SVR works by first transforming the input data into a higher-dimensional space using a kernel function. This is done to make the data more likely to be linearly separable in the high-dimensional space SVR uses a kernel function to transform the input data into a higher-dimensional space where the data is more likely to be linearly separable.

SVR works by first transforming the input data into a higher-dimensional space using a kernel function. This is done to make the data more likely to be linearly separable in the high-dimensional space

kernel function
The kernel function is a function that transforms the input data into a higher-dimensional space. There are many different kernel functions that can be used, but some of the most common ones include the linear kernel, the polynomial kernel, and the Gaussian kernel.
			linear, polynomial, and radial basis function (RBF)                                                                             

Parameters
C
Epsilon
gamma
kernal
shrinking
max_iter

Advantages of SVR:
Robust to outliers
Can handle both linear and non-linear regression problems
Good generalization performance

Disadvantages of SVR:
Can be computationally expensive to train
Hyperparameter tuning is important to achieve good performance

Evaluation Metrices
R-Squared (R2) - R-Squared is a number that explains the amount of variation that is explained/captured by the developed model. It always ranges between 0 & 1 . Overall, the higher the value of R-squared, the better the model fits the data.	    R2 = 1 – ( RSS/TSS )
Residual sum of Squares(RSS)-measure of the squred difference between the expected and the actual observed output.

Total Sum of Squares (TSS)-the sum of errors of the data points from the mean of the response variable

Mean Squred Error-the average squred diffrence between the predicted and actual value
		   MSE = (1/n) * Σ(actual - predicted)²
Root Mean Squared Error-RMSE provides a measure of the average magnitude of errors in the same units as 			the target variable.		RMSE = √(MSE)
		Lower RMSE indiactes better perfomance
Mean Absloute Error-the average absloute diffrence between the predicted and actual value
		     MAE = (1/n) * Σ|actual - predicted|