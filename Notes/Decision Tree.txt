Decision Tree
A decision tree is a tree-like structure that represents a series of decisions and their possible consequences. It is used in machine learning for classification and regression tasks. An example of a decision tree is a flowchart that helps a person decide what to wear based on the weather conditions

Types-Classification tree, Regression tree, Cost-complexity pruning tree, and Reduced Error Pruning tree.
Tree algorithm-ID3, C4.5, CART, and Random Forest
The goal of machine learning is to decrease uncertainty or disorders from the dataset and for this, we use decision trees.
Entropy-is nothing but the uncertainty in our dataset or measure of disorder.choose low entropy
Always remember that the higher the Entropy, the lower will be the purity and the higher will be the impurity.

Gini impurity measures the probability of misclassifying a randomly chosen element, while entropy measures the 	amount of disorder or randomness in a datase

Gini Index-works with the categorical target variable “Success” or “Failure”. It performs only Binary splits.
	   Lower Gini impurity indicates a better split
Information gain-which tells us how much the parent entropy has decreased after splitting it with some feature.
		Measures the reduction in entropy or impurity achieved by a split. A higher information gain 			suggests a better split.
		It is just entropy of the full dataset – entropy of the dataset given some feature.
Gain Ratio- measure that takes into account both the information gain and the number of outcomes of a feature to 	    determine the best feature to split on            Gain Ratio=information gain / splitinfo
Pruning-is another method that can help us avoid overfitting. It helps in improving the performance of the tree by 	cutting the nodes or sub-nodes which are not significant. Additionally, it removes the branches which have 	very low importance.
Pre-pruning – we can stop growing the tree earlier, which means we can prune/remove/cut a node if it has low importance while growing the tree.
Post-pruning – once our tree is built to its depth, we can start pruning the nodes based on their significance.

Advantages:-
Easy to understand and interpret.
Can handle both numerical and categorical data.
Require minimal data preprocessing.
Disadvantages:-
Prone to overfitting.
Can be unstable with small variations in the data.
May not always find the globally optimal tree.

Random Forest Algorithm
A Random Forest is an ensemble learning method that consists of multiple Decision Trees. It differs from a single Decision Tree in that it reduces overfitting by aggregating predictions from multiple trees, which are trained on different subsets of the data with bootstrapping and feature randomization.
Bagging– It creates a different training subset from sample training data with replacement & the final output is 	based on majority voting. For example,  Random Forest.
Boosting– It combines weak learners into strong learners by creating sequential models such that the final model 	has the highest accuracy. For example,  ADA BOOST, XG BOOST
	Boosting is one of the techniques that use the concept of ensemble learning. A boosting algorithm combines 	multiple simple models (also known as weak learners or base estimators) to generate the final output. It is 	done by building a model by using weak models in series.
OOB error - It is used as an estimate of the model's performance on unseen data without the need for a separate 	validation set.


Advantages-
Reduced overfitting.
Improved generalization.
Robust to noisy data.
Provides feature importance scores..
Disadvantages:-
Increased complexity.
Longer training time due to multiple trees.
May not perform well on very high-dimensional data.

SVM
SVM algorithm is used for both classification and regression tasks. It finds an optimal hyperplane to separate data points of different classes in a high-dimensional space.
SVM is considered one of the best algorithms because it can handle high-dimensional data, is effective in cases with limited training samples, and can handle non-linear classification using kernel functions
Support Vectors: These are the points that are closest to the hyperplane. A separating line will be defined with 		the help of these data points.
Margin: The margin in SVMs is the distance between the hyperplane and the closest data points from each class. 	large margin is considered a good margin
Hard margin-If our data is linearly separable, we go for a hard margin. 
Soft margin-but the margin is so small that the model becomes prone to overfitting or being too sensitive to 	 	   outliers. Also, in this case, we can opt for a larger margin by using soft margin SVM in order to help 	    the model generalize better.
Kernal:-we do is try converting this lower dimension space to a higher dimension space using some quadratic 	functions, SVM can handle non-linearly separable data using the kernel trick., Kernels (e.g., Polynomial, 	Radial Basis Function - RBF, Sigmoid) transform the input data into a higher-dimensional space where it 	might become linearly  separable., The kernel function replaces the dot product of feature vectors in the 	original space with a similarity measure in the higher-dimensional space.
C parameter-The C parameter in SVMs controls the trade-off between maximizing the margin and minimizing 		    classification errors
Advantages of SVM
Effective in high-dimensional spaces.
Robust against overfitting, especially with the right choice of kernel.
Suitable for both linear and non-linear classification

Disadvantages of SVM
Can be computationally expensive for large datasets.
Choice of kernel and hyperparameter tuning can be challenging.
Not well-suited for multi-class problems with many classes.

Accuracy:measures the rati o of correctly predicted instances to the total number of instances    
	 Accuracy = (TP + TN) / (TP + TN + FP + FN)
Precision: Precision measures the proportion of true positive predictions out of all positive predictions
		Precision = TP / (TP + FP)
Recall: measures the proportion of true positives that were correctly predicted by the model out of all actual 	positive instances It's useful when you want to minimize false negatives.   Recall = TP / (TP + FN)

F1-Score: The F1-Score is the harmonic mean of precision and recall to find a balance between false positives and 	  false negatives.	F1-Score = 2 * (Precision * Recall) / (Precision + Recall)
Confusion Matrix:confusion matrix provides a comprehensive view of a model's performance . It shows the number of 		true positives, true negatives, false positives, and false negatives.

AUC: area under the curve measure ability of a classifier to distingush between classes
ROC: reciver operator curve that plots the true postive rate against false postive rate

Grid Search: Grid search involves defining a grid of possible hyperparameter values and exhaustively trying all 	    combinations. It's a straightforward approach, but it can be computationally expensive, especially when    	     dealing with multiple hyperparameters or a large search space.

Random Search: Random search selects random combinations of hyperparameter values within predefined ranges. While 	       it may not cover all possible combinations like grid search, it has been shown to be more efficient 	       in terms of finding good combinations quickly.

GridSearchCV: GridSearchCV performs an exhaustive search over all possible combinations of hyperparameters within a 	      predefined grid
RandomizedSearchCV: RandomizedSearchCV, on the other hand, randomly samples a specified number of combinations from 	      the hyperparameter space. It does not explore all combinations but rather focuses on a random subset. 